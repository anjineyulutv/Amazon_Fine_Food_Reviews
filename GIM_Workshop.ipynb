{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwXFEeC9WHFaHotaA+ATBH",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anjineyulutv/Amazon_Fine_Food_Reviews/blob/master/GIM_Workshop.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70Hvr2TzEWJt",
        "outputId": "4a099aff-2afc-418f-8356-4cb2259395ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   income  spend_score  cluster_unscaled  cluster_scaled\n",
            "0   35795            6                 1               1\n",
            "1   20860           54                 1               2\n",
            "2   96820            4                 2               1\n",
            "3   74886           54                 2               0\n",
            "4   26265           93                 1               2\n"
          ]
        }
      ],
      "source": [
        "# --- Feature Scaling: Why it matters in clustering ---\n",
        "import pandas as pd\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Create dummy marketing data (income & spend score)\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'income': np.random.randint(20000, 100000, 50),\n",
        "    'spend_score': np.random.randint(1, 100, 50)\n",
        "})\n",
        "\n",
        "# Without scaling: Income dominates due to large numeric range\n",
        "kmeans_unscaled = KMeans(n_clusters=3, random_state=42).fit(df)\n",
        "df['cluster_unscaled'] = kmeans_unscaled.labels_\n",
        "\n",
        "# Apply Standard Scaling\n",
        "scaler = StandardScaler()\n",
        "scaled = scaler.fit_transform(df[['income','spend_score']])\n",
        "kmeans_scaled = KMeans(n_clusters=3, random_state=42).fit(scaled)\n",
        "df['cluster_scaled'] = kmeans_scaled.labels_\n",
        "\n",
        "print(df[['income','spend_score','cluster_unscaled','cluster_scaled']].head())\n",
        "\n",
        "# üí° Lesson: Without scaling, 'income' (20k‚Äì100k) dominates the clustering.\n",
        "# After scaling, both 'income' and 'spend_score' influence equally.\n",
        "# Always scale distance-based features in marketing segmentation."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# üö® Concept: Data Leakage ‚Äî When Your Model Knows the Future üö®\n",
        "# -------------------------------------------------------------\n",
        "# Marketing Scenario:\n",
        "# You're predicting customer churn using engagement data.\n",
        "# But your dataset mistakenly includes a feature that only exists AFTER the churn event (like \"last_month_purchase_drop\").\n",
        "# The model learns from it and shows unrealistically high performance.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----- Step 1: Create dummy marketing data -----\n",
        "n = 500\n",
        "data = pd.DataFrame({\n",
        "    'email_click_rate': np.random.beta(2,5,n),           # engagement level\n",
        "    'support_tickets': np.random.poisson(1.5, n),        # number of support issues\n",
        "    'last_login_days': np.random.randint(1, 60, n),      # days since last login\n",
        "})\n",
        "\n",
        "# True churn probability (hidden ground truth)\n",
        "true_prob = 0.3*data['email_click_rate'] + 0.2*(data['support_tickets']>2) - 0.01*data['last_login_days'] + np.random.normal(0,0.05,n)\n",
        "data['churn'] = (true_prob > 0.2).astype(int)\n",
        "\n",
        "# ----- Step 2: Add a 'leaked' feature -----\n",
        "# This feature correlates strongly with churn, but is actually from the future\n",
        "data['post_event_engagement'] = data['churn'] + np.random.normal(0,0.1,n)  # leakage!\n",
        "\n",
        "# Split data\n",
        "X = data.drop('churn', axis=1)\n",
        "y = data['churn']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# ----- Step 3: Model with leakage -----\n",
        "model_leak = LogisticRegression(max_iter=500)\n",
        "model_leak.fit(X_train, y_train)\n",
        "auc_leak = roc_auc_score(y_test, model_leak.predict_proba(X_test)[:,1])\n",
        "\n",
        "# ----- Step 4: Model without leakage -----\n",
        "X_train_noleak = X_train.drop('post_event_engagement', axis=1)\n",
        "X_test_noleak = X_test.drop('post_event_engagement', axis=1)\n",
        "\n",
        "model_clean = LogisticRegression(max_iter=500)\n",
        "model_clean.fit(X_train_noleak, y_train)\n",
        "auc_clean = roc_auc_score(y_test, model_clean.predict_proba(X_test_noleak)[:,1])\n",
        "\n",
        "# ----- Step 5: Compare Results -----\n",
        "print(f\"AUC with leakage:   {auc_leak:.3f}\")\n",
        "print(f\"AUC without leakage: {auc_clean:.3f}\")\n",
        "\n",
        "# üí° Life Lesson:\n",
        "# If your model performs *too perfectly*, double-check your features.\n",
        "# Leakage often happens when post-event or derived data sneaks into training."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7MClbSsEuuK",
        "outputId": "7fed8259-43fa-4f67-cc45-80c2dfed253c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC with leakage:   1.000\n",
            "AUC without leakage: 0.966\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Data Granularity Demo: Daily vs Monthly forecasting ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "np.random.seed(42)\n",
        "days = pd.date_range(\"2024-01-01\", periods=60)\n",
        "daily_sales = np.random.randint(100,500,60)\n",
        "df = pd.DataFrame({'date': days, 'sales': daily_sales})\n",
        "\n",
        "# Add monthly aggregate\n",
        "df['month'] = df['date'].dt.month\n",
        "monthly = df.groupby('month')['sales'].mean().reset_index()\n",
        "\n",
        "# Simple regression on day index (fine-grained)\n",
        "df['day_idx'] = np.arange(len(df))\n",
        "model_daily = LinearRegression().fit(df[['day_idx']], df['sales'])\n",
        "\n",
        "# Regression on monthly average (coarse-grained)\n",
        "monthly['month_idx'] = monthly.index\n",
        "model_monthly = LinearRegression().fit(monthly[['month_idx']], monthly['sales'])\n",
        "\n",
        "print(f\"Daily model coef: {model_daily.coef_[0]:.2f} | Monthly model coef: {model_monthly.coef_[0]:.2f}\")\n",
        "\n",
        "# üí° Lesson: Aggregating to months smooths variation ‚Äî hides weekend/holiday spikes.\n",
        "# In marketing, wrong granularity may erase actionable temporal insights."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fey9KOAPEvBV",
        "outputId": "b691ea47-9860-4e14-d261-cf12453dae1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Daily model coef: 0.80 | Monthly model coef: -9.01\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RFM Features: Recency, Frequency, Monetary ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# Generate fake transaction data\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'customer_id': np.random.choice(['C1','C2','C3','C4'], 20),\n",
        "    'purchase_date': [datetime(2025,1,1)+timedelta(days=int(x)) for x in np.random.randint(0,90,20)],\n",
        "    'amount': np.random.randint(50,500,20)\n",
        "})\n",
        "\n",
        "today = datetime(2025,4,1)\n",
        "rfm = df.groupby('customer_id').agg({\n",
        "    'purchase_date': lambda x: (today - x.max()).days,  # Recency\n",
        "    'customer_id': 'count',                             # Frequency\n",
        "    'amount': 'sum'                                     # Monetary\n",
        "}).rename(columns={'purchase_date':'Recency','customer_id':'Frequency','amount':'Monetary'})\n",
        "\n",
        "print(rfm)\n",
        "\n",
        "# üí° Lesson: RFM encodes business intuition into features.\n",
        "# Recency (how recently bought), Frequency (how often), Monetary (how much spent)\n",
        "# are powerful signals for marketing segmentation & campaign targeting."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C2-mBcoFEvOb",
        "outputId": "8e2f7563-0c4c-468e-b888-b627a5c99af1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "             Recency  Frequency  Monetary\n",
            "customer_id                              \n",
            "C1                27          4      1228\n",
            "C2                70          1       369\n",
            "C3                 3          9      2487\n",
            "C4                 2          6      1615\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Temporal Features in Marketing Data ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'date': pd.date_range(\"2024-01-01\", periods=10),\n",
        "    'visits': np.random.randint(100,500,10)\n",
        "})\n",
        "\n",
        "# Add temporal features\n",
        "df['day_of_week'] = df['date'].dt.dayofweek\n",
        "df['is_weekend'] = df['day_of_week'].isin([5,6]).astype(int)\n",
        "df['rolling_mean'] = df['visits'].rolling(window=3, min_periods=1).mean()\n",
        "\n",
        "print(df)\n",
        "\n",
        "# üí° Lesson: Adding day-of-week, weekend, and rolling trends helps capture seasonality.\n",
        "# In marketing, these features help models learn weekday vs. weekend traffic behavior."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LcxUrJmREva7",
        "outputId": "66ce9ccd-f97c-4523-c021-3fbe828ab5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        date  visits  day_of_week  is_weekend  rolling_mean\n",
            "0 2024-01-01     202            0           0    202.000000\n",
            "1 2024-01-02     448            1           0    325.000000\n",
            "2 2024-01-03     370            2           0    340.000000\n",
            "3 2024-01-04     206            3           0    341.333333\n",
            "4 2024-01-05     171            4           0    249.000000\n",
            "5 2024-01-06     288            5           1    221.666667\n",
            "6 2024-01-07     120            6           1    193.000000\n",
            "7 2024-01-08     202            0           0    203.333333\n",
            "8 2024-01-09     221            1           0    181.000000\n",
            "9 2024-01-10     314            2           0    245.666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öñÔ∏è Concept 6: Handling Imbalanced Targets ‚Äî Don‚Äôt Let the Minority Class Disappear\n",
        "# -----------------------------------------------------------------------------------\n",
        "# Marketing Scenario:\n",
        "# You‚Äôre predicting whether a customer will churn.\n",
        "# Only a small fraction actually churn (~10%).\n",
        "# The model might just predict \"no churn\" for everyone and still appear accurate.\n",
        "# Let's see why using class weights (or other techniques) is essential.\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "# ----- Step 1: Create an imbalanced dataset -----\n",
        "n = 800\n",
        "data = pd.DataFrame({\n",
        "    'usage_hours': np.random.exponential(10, n),     # engagement level\n",
        "    'email_opens': np.random.poisson(3, n),          # marketing interactions\n",
        "    'support_calls': np.random.poisson(1, n),        # customer support contacts\n",
        "})\n",
        "\n",
        "# 10% churners only\n",
        "data['churn'] = (np.random.rand(n) < 0.1).astype(int)\n",
        "\n",
        "# Slight pattern: churners tend to have lower usage\n",
        "data.loc[data['churn'] == 1, 'usage_hours'] *= 0.5\n",
        "data.loc[data['churn'] == 1, 'email_opens'] *= 0.7\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.drop('churn', axis=1), data['churn'], test_size=0.3, random_state=42, stratify=data['churn']\n",
        ")\n",
        "\n",
        "# ----- Step 2: Train model without handling imbalance -----\n",
        "model_plain = LogisticRegression(max_iter=500)\n",
        "model_plain.fit(X_train, y_train)\n",
        "pred_plain = model_plain.predict(X_test)\n",
        "auc_plain = roc_auc_score(y_test, model_plain.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# ----- Step 3: Train model with class weights -----\n",
        "model_weighted = LogisticRegression(max_iter=500, class_weight='balanced')\n",
        "model_weighted.fit(X_train, y_train)\n",
        "pred_weighted = model_weighted.predict(X_test)\n",
        "auc_weighted = roc_auc_score(y_test, model_weighted.predict_proba(X_test)[:, 1])\n",
        "\n",
        "# ----- Step 4: Compare performance -----\n",
        "print(\"Without Handling Imbalance:\")\n",
        "print(classification_report(y_test, pred_plain, digits=3))\n",
        "print(f\"AUC: {auc_plain:.3f}\\n\")\n",
        "\n",
        "print(\"With Class Weight Balancing:\")\n",
        "print(classification_report(y_test, pred_weighted, digits=3))\n",
        "print(f\"AUC: {auc_weighted:.3f}\")\n",
        "\n",
        "# üí° Life Lesson:\n",
        "# High accuracy doesn‚Äôt always mean a good model.\n",
        "# In marketing churn, fraud, or rare-event prediction ‚Äî the *minority class* matters most.\n",
        "# Always inspect recall/precision for that class or use class weights, SMOTE, or stratified sampling."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AsHWV3gwHnFi",
        "outputId": "313e5e81-1b0d-41f8-c377-9b7c0eb5974b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Handling Imbalance:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.904     1.000     0.950       217\n",
            "           1      0.000     0.000     0.000        23\n",
            "\n",
            "    accuracy                          0.904       240\n",
            "   macro avg      0.452     0.500     0.475       240\n",
            "weighted avg      0.818     0.904     0.859       240\n",
            "\n",
            "AUC: 0.715\n",
            "\n",
            "With Class Weight Balancing:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0      0.951     0.627     0.756       217\n",
            "           1      0.165     0.696     0.267        23\n",
            "\n",
            "    accuracy                          0.633       240\n",
            "   macro avg      0.558     0.661     0.511       240\n",
            "weighted avg      0.876     0.633     0.709       240\n",
            "\n",
            "AUC: 0.714\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3232789135.py:30: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise an error in a future version of pandas. Value '[1.4 1.4 2.1 0.7 2.1 2.1 0.7 0.7 2.1 1.4 5.6 1.4 1.4 0.  2.1 0.7 2.1 2.8\n",
            " 2.1 0.  3.5 0.7 2.1 4.2 0.7 1.4 0.  2.8 2.8 2.8 2.1 2.8 1.4 0.7 4.9 2.8\n",
            " 4.2 4.9 2.1 2.8 3.5 0.7 3.5 2.1 2.8 0.7 1.4 2.1 3.5 2.1 2.8 1.4 3.5 0.7\n",
            " 1.4 4.2 2.1 4.9 2.8 1.4 0.7 2.8 0.7 3.5 5.6 2.1 2.1 2.1 2.1 2.8 0.7 1.4\n",
            " 2.8 0.7 0.7 0.7]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
            "  data.loc[data['churn'] == 1, 'email_opens'] *= 0.7\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- One-Hot Encoding vs Label Encoding ---\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "\n",
        "df = pd.DataFrame({'channel': ['Email','Social','Affiliate','Social','Email']})\n",
        "\n",
        "# Label Encoding (WRONG for non-ordinal data)\n",
        "le = LabelEncoder()\n",
        "df['label_encoded'] = le.fit_transform(df['channel'])\n",
        "\n",
        "# One-Hot Encoding (Correct)\n",
        "ohe = OneHotEncoder(sparse_output=False)\n",
        "encoded = pd.DataFrame(ohe.fit_transform(df[['channel']]), columns=ohe.get_feature_names_out(['channel']))\n",
        "df = pd.concat([df, encoded], axis=1)\n",
        "\n",
        "print(df)\n",
        "\n",
        "# üí° Lesson: Label encoding imposes fake numeric order (‚ÄúSocial‚Äù>‚ÄúEmail‚Äù).\n",
        "# One-hot keeps equality among categories, preserving campaign semantics."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ttv3y2hQHnWz",
        "outputId": "3f3ef339-fa65-479f-db4f-368a0f1cd4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     channel  label_encoded  channel_Affiliate  channel_Email  channel_Social\n",
            "0      Email              1                0.0            1.0             0.0\n",
            "1     Social              2                0.0            0.0             1.0\n",
            "2  Affiliate              0                1.0            0.0             0.0\n",
            "3     Social              2                0.0            0.0             1.0\n",
            "4      Email              1                0.0            1.0             0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Feature Selection Importance ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Simulated marketing dataset with redundant features\n",
        "np.random.seed(42)\n",
        "df = pd.DataFrame({\n",
        "    'age': np.random.randint(18,60,100),\n",
        "    'income': np.random.randint(30000,100000,100),\n",
        "    'click_rate': np.random.rand(100),\n",
        "    'random_noise': np.random.rand(100),  # useless feature\n",
        "    'target': np.random.randint(0,2,100)\n",
        "})\n",
        "\n",
        "X = df.drop('target', axis=1)\n",
        "y = df['target']\n",
        "\n",
        "model = RandomForestClassifier().fit(X, y)\n",
        "feat_imp = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
        "print(feat_imp)\n",
        "\n",
        "# üí° Lesson: Not all features help. Drop low-importance ones to simplify model.\n",
        "# In marketing, too many noisy metrics ‚Üí overfit and confusion."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbtngamHHnoX",
        "outputId": "e4144d97-7a5a-4c64-fe2f-0793c54e9215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "click_rate      0.299527\n",
            "income          0.242154\n",
            "random_noise    0.238246\n",
            "age             0.220074\n",
            "dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Handling Missing Values ---\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    'customer_id': ['C1','C2','C3','C4'],\n",
        "    'age': [25, np.nan, 30, np.nan],\n",
        "    'spend_score': [80, 70, np.nan, 60]\n",
        "})\n",
        "\n",
        "# Mean imputation for numeric columns\n",
        "imputer = SimpleImputer(strategy='mean')\n",
        "df[['age','spend_score']] = imputer.fit_transform(df[['age','spend_score']])\n",
        "\n",
        "print(df)\n",
        "\n",
        "# üí° Lesson: Missing data is unavoidable ‚Äî impute wisely.\n",
        "# Mean/median fills are simple; advanced methods (KNN, model-based) are better for large gaps.\n",
        "# Always inspect the cause before imputing."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c77M62NtHn9k",
        "outputId": "a1b418f0-36ed-4224-84f0-c98522917290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  customer_id   age  spend_score\n",
            "0          C1  25.0         80.0\n",
            "1          C2  27.5         70.0\n",
            "2          C3  30.0         70.0\n",
            "3          C4  27.5         60.0\n"
          ]
        }
      ]
    }
  ]
}